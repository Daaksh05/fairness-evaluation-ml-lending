Fairness Evaluation of ML Models in Financial Lending Data

This project investigates bias and fairness in machine learning models used for financial lending decisions such as loan approval and credit risk assessment. The goal is to evaluate whether models behave equitably across demographic groups (e.g., gender, age, socioeconomic groups) and to apply fairness-enhancing mitigation strategies.

The project is designed as a computational social science and responsible AI study, integrating machine learning, fairness metrics, interpretability, and ethical evaluation.

ğŸš€ Project Objectives

Build ML models for predicting loan approval / credit risk

Evaluate fairness using statistical and group fairness metrics

Identify disparate model behavior across sensitive attributes

Apply bias mitigation strategies (pre-, in-, post-processing)

Provide actionable insights for ethical and fair model deployment

ğŸ“‚ Project Structure
fairness-evaluation-ml-lending/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original datasets
â”‚   â”œâ”€â”€ processed/          # Cleaned datasets
â”‚   â””â”€â”€ sample/             # Sample data for testing
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â”œâ”€â”€ 04_fairness_metrics.ipynb
â”‚   â””â”€â”€ 05_bias_mitigation.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # Data loading & preprocessing
â”‚   â”œâ”€â”€ models/             # Model training & evaluation
â”‚   â”œâ”€â”€ fairness/           # Fairness metrics & mitigation
â”‚   â””â”€â”€ utils/              # Config & helper functions
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_training.py
â”‚   â”œâ”€â”€ run_fairness_check.py
â”‚   â”œâ”€â”€ run_mitigation.py
â”‚   â””â”€â”€ deploy_app.py
â”‚
â”œâ”€â”€ reports/                # Visualizations, logs, summary
â”œâ”€â”€ models/                 # Trained model artifacts
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸ§  Methodology
1ï¸âƒ£ Dataset

You may use datasets such as:

Lending Club dataset

HMDA (Home Mortgage Disclosure Act) dataset

Any financial lending dataset with sensitive attributes

Features include:

Income, loan amount, credit score

Employment length, DTI ratio

Sensitive attributes (gender, age, etc.)

2ï¸âƒ£ Model Training

Machine learning models explored include:

Logistic Regression

Random Forest

XGBoost

Target variable:

loan_status âˆˆ { approved, rejected }

3ï¸âƒ£ Fairness Metrics

The project evaluates:

Demographic Parity

Equal Opportunity

Disparate Impact Ratio

False Positive Rate (FPR) gap

False Negative Rate (FNR) gap

Tools used:

fairlearn

sklearn

custom fairness metric functions

4ï¸âƒ£ Bias Mitigation Techniques
Pre-processing

Reweighing

Removing or encoding sensitive attributes

In-processing

Fairness-constrained optimization

Post-processing

Threshold adjustment

Reject option classification

ğŸ“Š Results Summary (Example)

The model showed a disparate impact ratio below 0.8, indicating bias

Threshold adjustment improved fairness by 15â€“20%

Reweighing reduced approval rate disparity between demographic groups

(This section can be updated once your results are generated.)

ğŸ›  Tech Stack

Python 3.10+

Pandas, NumPy

Scikit-learn

Fairlearn

Matplotlib / Seaborn

Jupyter Notebook

Streamlit (optional deployment)
